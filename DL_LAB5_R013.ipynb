{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course : Deep Learning  \n",
    "Program : MBA Tech AI  \n",
    "Sem : ðŸ‡»  \n",
    "Academic year : 2024-25  \n",
    "Instructor : Radhika Chaperneri\n",
    "Name : Dhruv Pithadia  \n",
    "Roll No : R013\n",
    "Batch : B1\n",
    "Date : 29-08-2024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mlp = X_train.reshape(X_train.shape[0], img_rows*img_cols)\n",
    "Y_train_mlp = Y_train\n",
    "\n",
    "X_test_mlp = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "Y_test_mlp = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (60000, 28, 28) Y_train shape (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape\", X_train.shape, \"Y_train shape\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = 2 Pullover\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x197ad9e4100>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjG0lEQVR4nO3df3DU9b3v8ddufmwCJBtDyC8JNKBCKz/aUkm5KsWSAdIzXlBux193BjxeGG1witRq06uiPZ2bFudaR4fi3JkW6oz4q1dg9HToUTShtgELyuFQbUrSVKCQINRkQ0J+bPZz/+CY3vBD+v6a5JOE52NmZ8juvvL97Dff5ZVvdvNOyDnnBADAIAv7XgAA4NJEAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwItn3As6WSCR09OhRZWRkKBQK+V4OAMDIOafW1lYVFhYqHL7wec6QK6CjR4+qqKjI9zIAAJ/R4cOHNX78+AvePuQKKCMjQ5J0nb6hZKV4Xk0/CnI2NwKnJCVPuNycaVxgz1xxy0FzRpKOtEbNmab6HHMm3GU/Hnoye8yZf5q535yRpH/9j+nmzFXfs+/zROspc2ZQ8bwNJK5uva1f9f5/fiEDVkDr16/X448/rsbGRs2cOVNPP/20Zs+efdHcJz92S1aKkkOXeAFp5B3IyeGIOZOUmmbOpIxONWckKTlhX1843b6+cNh+PLh0ewGljgn2HArymJJD9n2eGOrPcZ63wfznLrjYyygD8iaEF198UWvWrNHatWv17rvvaubMmVq4cKGOHz8+EJsDAAxDA1JATzzxhFasWKE777xTX/jCF/TMM89o1KhR+vnPfz4QmwMADEP9XkBdXV3au3evSktL/76RcFilpaWqqak55/6dnZ2KxWJ9LgCAka/fC+jEiRPq6elRXl5en+vz8vLU2Nh4zv0rKysVjUZ7L7wDDgAuDd5/EbWiokItLS29l8OHD/teEgBgEPT7u+BycnKUlJSkpqamPtc3NTUpPz//nPtHIhFFIvZ3HgEAhrd+PwNKTU3VrFmztGPHjt7rEomEduzYoTlz5vT35gAAw9SA/B7QmjVrtGzZMn3lK1/R7Nmz9eSTT6qtrU133nnnQGwOADAMDUgB3XLLLfroo4/0yCOPqLGxUV/84he1ffv2c96YAAC4dIWcG1pzI2KxmKLRqOZp8dCdhDCEx3Mkj7ePrfnggQvPavo0//XavebMZcnt5kxTV6Y5k5HcYc5I0t3Zb5szxSljAm3L6lTC/ph+1R7sm76dLVPNmXGprebMB6fOfV34YvbsusqcmfJ4gzkjSfHGpovfCeeIu25VaZtaWlqUmXnh56/3d8EBAC5NFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPBiQKZho3+EZ37enPnG8/ZhmmNb7EMkJenPp3LMmdNx+4DZ7p4kc6atK9WckaRf/uFL5syo0Z3mTE+P/Xu/ri770zUlpceckaQJ2R+bM4eSLzNnxiTb99386//dnPnommADY5t+Yf8bZmN/VhNoW5cizoAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBdOwg3BuUDbzcWW3OVPTPNmcaYhlmzOSlJYcN2cSLmTOdAaYhh0KBfsaBZls3dlpfxrFA0y2Tg4w2TpjVIc5IwWbWt7ZY39Msc40cyYpnGHOjE7pMmck6Yp/rjVnYq/Yp4L3fGyfPj4ScAYEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF4wjHSQJE/6nDkzfewxc+ZwW5Y5MyrFPvRUkjrj9sMnO63dnBmXbh96mhxKmDOSFHf278m6Agzh7ErYB6xmpZ42ZwrSWswZSepM2IeRnu4JMMA0Yd93Taftw0iDDD2VpLy0VnOm9vaZ5kzu+t+ZMyMBZ0AAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AXDSAdJPDfTnLk2ah9Q+GZiqjmTmdxpzkhSYaTZnGlPpJoz2clt5ky3sw/7lKRwgCGmKaEecyYRYOhpJGwfGpukYENZu539v4Yg+y7I0FPZn0ra1zreHpKUmWwfANsxzz7AVOvtkZGAMyAAgBcUEADAi34voEcffVShUKjPZepU+4+FAAAj24C8BnT11VfrjTfe+PtGknmpCQDQ14A0Q3JysvLz8wfiUwMARogBeQ3o4MGDKiws1KRJk3THHXfo0KFDF7xvZ2enYrFYnwsAYOTr9wIqKSnRpk2btH37dm3YsEENDQ26/vrr1dp6/rcmVlZWKhqN9l6Kior6e0kAgCGo3wuorKxM3/zmNzVjxgwtXLhQv/rVr9Tc3KyXXnrpvPevqKhQS0tL7+Xw4cP9vSQAwBA04O8OyMrK0lVXXaW6urrz3h6JRBSJRAZ6GQCAIWbAfw/o1KlTqq+vV0FBwUBvCgAwjPR7Ad1///2qrq7WX/7yF/3ud7/TTTfdpKSkJN122239vSkAwDDW7z+CO3LkiG677TadPHlS48aN03XXXaddu3Zp3Lhx/b0pAMAw1u8F9MILL/T3pxwRPvrSaHMmLWQfPvlfovXmTJBhmmdycXPmRNw+SfLtv002Z/79ULDhk0mH0syZ5LaQfTsB5r+mtDlzJsD8UklST8T+mJqvth8P3/7av5kzx7vsx9BVo4+bM5I0IfWEOfObUfbj9VLFLDgAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8CLknLNPOBxAsVhM0WhU87RYyaEU38vxKunKSeZM3Z155kzk8y3mjCRd/r+SzBn3+/8ItK3BkpRpH3QZyhhjzrjR6eZMItOe6UkP9hxKbrVPS03sez/QtqxmvZcwZxZkHgi0rb/GLzNn/tB+uTmz90sj61wg7rpVpW1qaWlR5qc8p0bWowYADBsUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4kex7AZeKPz0z2x4KMKe8oNoeCu2zT4CWpK7L4ubMrR8cN2eSZJ9+XN+Ra85I0vsx+8Tpv7bap2F3xgNMEnf2/RAKdZgzkpSXccqcuWv8h+bML4/PMmfe/R/2ie/7WiabM5LkjjaZM4n29kDbuhRxBgQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXoSccwFGXg6cWCymaDSqeVqs5FCK7+X0m7b/VmLOHL3Bvp3kbPvwyXVf+b/2DUn6zr/+d3Om4Df2w60zav8+KRZs9qTiowM8HYJEku0hlxJg0GxXyJyRpFDCnsv6wJ5JbbU/po+XtJkz8e5gc5cTzanmzPe+/qo5s+3rM8yZ+LFGc2awxF23qrRNLS0tysy88LBjzoAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAuGkQ6SWe8lzJlTPRFzZu+JInNmbHq7OSNJs7IOmTNrx70faFtWpxL2oayS9LdE3JzpcPYhnD0BMu3OPlAzLdRjzkhSNGzPjU8eY878oeu0OfM/P1xizhw8kWPOSFLav114kOaFdI+xf20L/vfvzJmhjGGkAIAhjQICAHhhLqCdO3fqxhtvVGFhoUKhkLZu3drnduecHnnkERUUFCg9PV2lpaU6ePBgf60XADBCmAuora1NM2fO1Pr16897+7p16/TUU0/pmWee0e7duzV69GgtXLhQHR3BfiYPABiZzK9qlpWVqays7Ly3Oef05JNP6qGHHtLixYslSc8++6zy8vK0detW3XrrrZ9ttQCAEaNfXwNqaGhQY2OjSktLe6+LRqMqKSlRTU3NeTOdnZ2KxWJ9LgCAka9fC6ix8czfKM/Ly+tzfV5eXu9tZ6usrFQ0Gu29FBXZ30YMABh+vL8LrqKiQi0tLb2Xw4cP+14SAGAQ9GsB5efnS5Kampr6XN/U1NR729kikYgyMzP7XAAAI1+/FlBxcbHy8/O1Y8eO3utisZh2796tOXPm9OemAADDnPldcKdOnVJdXV3vxw0NDdq3b5+ys7M1YcIErV69Wj/84Q915ZVXqri4WA8//LAKCwu1ZMmS/lw3AGCYMxfQnj17dMMNN/R+vGbNGknSsmXLtGnTJj3wwANqa2vTypUr1dzcrOuuu07bt29XWlpa/60aADDsMYx0kPz5x/YfQc66rtacuTX3HXPm/ne+ac5IUuRAujnTMc4+lHX0EftPil2SOSJJStjnfaon3f4UCro+q1DcPhhTkpLtM0IV7rZnuu3zS9VR1GXO1JX9H/uGJN15aJ458+zEneZM6e3/bM4kVb1rzgwWhpECAIY0CggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvAgw+xdBpE9pNmc+7hhlzvwmdpU5M/r39qnWknS6pM2c+acr3zdnEs7+fVIkyGjmgLoDjLYO8pjCIfsk8XAo2LD7SDhuzsQT9sf07t+KzJnYLwvNmR9eM82ckaR3Dk80Z6Y33m7OFL1bd/E7naXHnBh6OAMCAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8YRjpI5l7+Z3MmPanLnFkU3W/O1DTONmckKXY6xZw53ZNqzvy1PWrOJIftgzslqTNuf0qkJNnHQgYZ3OlcyJwJBRxGmpNmHzTbHrcfD1dnNZozv2+3DyMtjhw3ZyTpC/n29U0ec8KcOfC5KeaM9sfsmSGGMyAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IJhpIMkOWwfWPm3rtHmTIezD4RMjdnXJkkp6d3mTNzZv+dJDbDvUpPi5owkhWUf3hnkaxsPJZkz4ZB9wGrc2bcjSSkBHtOYFPv6ImH7MTTqo2Bf2yCmZjSZM6MCDBFun5BpzqTZ5w4POZwBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXDCMdJCkh+3DHcMg+GLPb2b+kkRMd5owkpaXbh0J2J+zDMYMM+0y4kDkTVJBtJWTPBPlu8XTcPpxWkrpT7F+n9CT7YNHksH2AadqRVnPmRNw+7FOSOhMBnk9h+/OiK9P+1U0zJ4YezoAAAF5QQAAAL8wFtHPnTt14440qLCxUKBTS1q1b+9y+fPlyhUKhPpdFixb113oBACOEuYDa2to0c+ZMrV+//oL3WbRokY4dO9Z7ef755z/TIgEAI4/5FbaysjKVlZV96n0ikYjy8/MDLwoAMPINyGtAVVVVys3N1ZQpU3TPPffo5MmTF7xvZ2enYrFYnwsAYOTr9wJatGiRnn32We3YsUM//vGPVV1drbKyMvX0nP+ttJWVlYpGo72XoqKi/l4SAGAI6vffA7r11lt7/z19+nTNmDFDkydPVlVVlebPn3/O/SsqKrRmzZrej2OxGCUEAJeAAX8b9qRJk5STk6O6urrz3h6JRJSZmdnnAgAY+Qa8gI4cOaKTJ0+qoKBgoDcFABhGzD+CO3XqVJ+zmYaGBu3bt0/Z2dnKzs7WY489pqVLlyo/P1/19fV64IEHdMUVV2jhwoX9unAAwPBmLqA9e/bohhtu6P34k9dvli1bpg0bNmj//v36xS9+oebmZhUWFmrBggX6l3/5F0Uikf5bNQBg2DMX0Lx58+TchYdk/vrXv/5MC8LfBRpq6AIM+zx03JyRpIy00YFygyHIIFdJirsAQyEDDEtNVoBMgMGdSSF7RpK6AgyNDXK8BhHq6DRnwgH3Q5B9HmSAaSJp8IbnDiXMggMAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAX/f4nuXF+CTc4026TZJ8CHW9sCrSttOQJ5kyQ/RAPMJk56PTjzh77UyI5wLYSsu+HRM/gfb/Y0ZNizgTZD0myZ9zoNHPmT+355owkZSW3B8pZ9dgf0ojAGRAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeMEwUgQWTT1tzsSd/XueIINFk8PBhpEmBRxiahVoOG2ASE+A/S1JCWffD6fiEXMmJdxjzvSMTjVnqj68wpyRpNuv2mPOtMTTzZlBmlU85HAGBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeMIx0kBw+fZk5k58WM2dSQnFzJqixkXZzpjXAwMpEgIGa8cGZKSpJSgSYEhoOOXtG9kyQYZ9SsGGpp+Mp5kyQx+TC9rV1HhljzkjSqKld5szHbpQ545LMkRGBMyAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IJhpAGE09LMmSDDHVNC9kGSdZ355kxQo5M7zZm2eOoArORcQQaYStKoZPvwya6E/WkUZBhpEGlJ3YFyQR5TT8K+z4MMcnUp9u2MPhTseBiT1GHOdCbsQ1kTKfb9MBJwBgQA8IICAgB4YSqgyspKXXPNNcrIyFBubq6WLFmi2traPvfp6OhQeXm5xo4dqzFjxmjp0qVqamrq10UDAIY/UwFVV1ervLxcu3bt0uuvv67u7m4tWLBAbW1tvfe577779Oqrr+rll19WdXW1jh49qptvvrnfFw4AGN5MrzRu3769z8ebNm1Sbm6u9u7dq7lz56qlpUU/+9nPtHnzZn3961+XJG3cuFGf//zntWvXLn31q1/tv5UDAIa1z/QaUEtLiyQpOztbkrR37151d3ertLS09z5Tp07VhAkTVFNTc97P0dnZqVgs1ucCABj5AhdQIpHQ6tWrde2112ratGmSpMbGRqWmpiorK6vPffPy8tTY2Hjez1NZWaloNNp7KSoqCrokAMAwEriAysvLdeDAAb3wwgufaQEVFRVqaWnpvRw+fPgzfT4AwPAQ6BdRV61apddee007d+7U+PHje6/Pz89XV1eXmpub+5wFNTU1KT///L8gGYlEFIlEgiwDADCMmc6AnHNatWqVtmzZojfffFPFxcV9bp81a5ZSUlK0Y8eO3utqa2t16NAhzZkzp39WDAAYEUxnQOXl5dq8ebO2bdumjIyM3td1otGo0tPTFY1Gddddd2nNmjXKzs5WZmam7r33Xs2ZM4d3wAEA+jAV0IYNGyRJ8+bN63P9xo0btXz5cknST37yE4XDYS1dulSdnZ1auHChfvrTn/bLYgEAI4epgJy7+ADFtLQ0rV+/XuvXrw+8qKHuH9kPZwsyjDQ9wCDJnSevNGekYJMqIuG4ORNk+GQ84GDRIMIB1hdksGhY9kyQ/RDvCTZvODmcMGeCHOMdAQZ3dkXtjym7NthQ1tFh+8DdQANWL81ZpMyCAwD4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBfBRuXCLBFgknFKqMec+WNTrjkzMeA07CDrCzIxeVRylzmTHLJPc5akSJJ9wnd3IinQtqzCAR5TkONOkroCPKYgU8GD6Ija1zZ2X3OgbaWE7MdDkEnnAQZojwicAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFwwjHSSJANMGgwz77D4y2pwJqrl7lDlT97ccc6b1VLo5k+gZvOmOrifA93Fh+8DKUJBhnwF3QyhALiXVPrgzK7XdnOkeE2BxdYfsGUlJAQaLdgcYAJu4RP8n5gwIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALy4REfgfTahAJMawwGGGgaRcmrwhnBmpdgHSY5K7TZnutLsh+n4rGZzRpI6e+zb6upJMmcG66sUDjLAVFJSOGHOnDhlH4RbkBYzZ3bn2x9Toq3NnJGkrCR7Lj3JfownUsyREYEzIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwgmGkQaTYJwe2xVPNmfaEPeMGbxapXtx+nTkTz+wxZyIn7MM+G5IyzRlJCtmXF4izP6RgX9uAx0PIPotUobh9Yy/HvmzOjN87SF8kSW2JiDnTlbD/t+ou0VOBS/RhAwB8o4AAAF6YCqiyslLXXHONMjIylJubqyVLlqi2trbPfebNm6dQKNTncvfdd/frogEAw5+pgKqrq1VeXq5du3bp9ddfV3d3txYsWKC2s/7Y04oVK3Ts2LHey7p16/p10QCA4c/0atn27dv7fLxp0ybl5uZq7969mjt3bu/1o0aNUn5+fv+sEAAwIn2m14BaWlokSdnZ2X2uf+6555STk6Np06apoqJC7e0X/tPNnZ2disVifS4AgJEv8NuwE4mEVq9erWuvvVbTpk3rvf7222/XxIkTVVhYqP379+vBBx9UbW2tXnnllfN+nsrKSj322GNBlwEAGKYCF1B5ebkOHDigt99+u8/1K1eu7P339OnTVVBQoPnz56u+vl6TJ08+5/NUVFRozZo1vR/HYjEVFRUFXRYAYJgIVECrVq3Sa6+9pp07d2r8+PGfet+SkhJJUl1d3XkLKBKJKBKx/7IXAGB4MxWQc0733nuvtmzZoqqqKhUXF180s2/fPklSQUFBoAUCAEYmUwGVl5dr8+bN2rZtmzIyMtTY2ChJikajSk9PV319vTZv3qxvfOMbGjt2rPbv36/77rtPc+fO1YwZMwbkAQAAhidTAW3YsEHSmV82/f9t3LhRy5cvV2pqqt544w09+eSTamtrU1FRkZYuXaqHHnqo3xYMABgZzD+C+zRFRUWqrq7+TAsCAFwamIYdQHjMaHMmKcB44ZQAo5m7owHGGAc06Xs1g7YtwIdEgF+VDOvTv1E/n+6oPTMSMIwUAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALxgGGkA8WON5syf6q8xZ+qO5Zoz434/iN9ThEKDs52LTGEHBsqaX99hzlw28WNzJmffpXmMcwYEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8GHKz4Nx/zv2Kq1saQeOREqc7zJmQ4uZMT5c5orjrtockScyCw8gW5Hnb095pz3TbtxP8eTvw4jqzNneR527IXeweg+zIkSMqKiryvQwAwGd0+PBhjR8//oK3D7kCSiQSOnr0qDIyMhQ6a9pyLBZTUVGRDh8+rMzMTE8r9I/9cAb74Qz2wxnshzOGwn5wzqm1tVWFhYUKhy/8Ss+Q+xFcOBz+1MaUpMzMzEv6APsE++EM9sMZ7Icz2A9n+N4P0Wj0ovfhTQgAAC8oIACAF8OqgCKRiNauXatIJOJ7KV6xH85gP5zBfjiD/XDGcNoPQ+5NCACAS8OwOgMCAIwcFBAAwAsKCADgBQUEAPBi2BTQ+vXr9bnPfU5paWkqKSnRO++843tJg+7RRx9VKBTqc5k6darvZQ24nTt36sYbb1RhYaFCoZC2bt3a53bnnB555BEVFBQoPT1dpaWlOnjwoJ/FDqCL7Yfly5efc3wsWrTIz2IHSGVlpa655hplZGQoNzdXS5YsUW1tbZ/7dHR0qLy8XGPHjtWYMWO0dOlSNTU1eVrxwPhH9sO8efPOOR7uvvtuTys+v2FRQC+++KLWrFmjtWvX6t1339XMmTO1cOFCHT9+3PfSBt3VV1+tY8eO9V7efvtt30sacG1tbZo5c6bWr19/3tvXrVunp556Ss8884x2796t0aNHa+HCherosA94HMouth8kadGiRX2Oj+eff34QVzjwqqurVV5erl27dun1119Xd3e3FixYoLa2tt773HfffXr11Vf18ssvq7q6WkePHtXNN9/scdX97x/ZD5K0YsWKPsfDunXrPK34AtwwMHv2bFdeXt77cU9PjyssLHSVlZUeVzX41q5d62bOnOl7GV5Jclu2bOn9OJFIuPz8fPf444/3Xtfc3OwikYh7/vnnPaxwcJy9H5xzbtmyZW7x4sVe1uPL8ePHnSRXXV3tnDvztU9JSXEvv/xy730++OADJ8nV1NT4WuaAO3s/OOfc1772Nfftb3/b36L+AUP+DKirq0t79+5VaWlp73XhcFilpaWqqanxuDI/Dh48qMLCQk2aNEl33HGHDh065HtJXjU0NKixsbHP8RGNRlVSUnJJHh9VVVXKzc3VlClTdM899+jkyZO+lzSgWlpaJEnZ2dmSpL1796q7u7vP8TB16lRNmDBhRB8PZ++HTzz33HPKycnRtGnTVFFRofb2dh/Lu6AhN4z0bCdOnFBPT4/y8vL6XJ+Xl6c//vGPnlblR0lJiTZt2qQpU6bo2LFjeuyxx3T99dfrwIEDysjI8L08LxobGyXpvMfHJ7ddKhYtWqSbb75ZxcXFqq+v1/e//32VlZWppqZGSUlJvpfX7xKJhFavXq1rr71W06ZNk3TmeEhNTVVWVlaf+47k4+F8+0GSbr/9dk2cOFGFhYXav3+/HnzwQdXW1uqVV17xuNq+hnwB4e/Kysp6/z1jxgyVlJRo4sSJeumll3TXXXd5XBmGgltvvbX339OnT9eMGTM0efJkVVVVaf78+R5XNjDKy8t14MCBS+J10E9zof2wcuXK3n9Pnz5dBQUFmj9/vurr6zV58uTBXuZ5DfkfweXk5CgpKemcd7E0NTUpPz/f06qGhqysLF111VWqq6vzvRRvPjkGOD7ONWnSJOXk5IzI42PVqlV67bXX9NZbb/X58y35+fnq6upSc3Nzn/uP1OPhQvvhfEpKSiRpSB0PQ76AUlNTNWvWLO3YsaP3ukQioR07dmjOnDkeV+bfqVOnVF9fr4KCAt9L8aa4uFj5+fl9jo9YLKbdu3df8sfHkSNHdPLkyRF1fDjntGrVKm3ZskVvvvmmiouL+9w+a9YspaSk9DkeamtrdejQoRF1PFxsP5zPvn37JGloHQ++3wXxj3jhhRdcJBJxmzZtcu+//75buXKly8rKco2Njb6XNqi+853vuKqqKtfQ0OB++9vfutLSUpeTk+OOHz/ue2kDqrW11b333nvuvffec5LcE0884d577z334YcfOuec+9GPfuSysrLctm3b3P79+93ixYtdcXGxO336tOeV969P2w+tra3u/vvvdzU1Na6hocG98cYb7stf/rK78sorXUdHh++l95t77rnHRaNRV1VV5Y4dO9Z7aW9v773P3Xff7SZMmODefPNNt2fPHjdnzhw3Z84cj6vufxfbD3V1de4HP/iB27Nnj2toaHDbtm1zkyZNcnPnzvW88r6GRQE559zTTz/tJkyY4FJTU93s2bPdrl27fC9p0N1yyy2uoKDApaamussvv9zdcsstrq6uzveyBtxbb73lJJ1zWbZsmXPuzFuxH374YZeXl+cikYibP3++q62t9bvoAfBp+6G9vd0tWLDAjRs3zqWkpLiJEye6FStWjLhv0s73+CW5jRs39t7n9OnT7lvf+pa77LLL3KhRo9xNN93kjh075m/RA+Bi++HQoUNu7ty5Ljs720UiEXfFFVe47373u66lpcXvws/Cn2MAAHgx5F8DAgCMTBQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADw4v8BPtXud5v1EJIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fashion_mnist_labels = [\"T-shirt/top\",  # index 0\n",
    "                        \"Trouser\",      # index 1\n",
    "                        \"Pullover\",     # index 2\n",
    "                        \"Dress\",        # index 3\n",
    "                        \"Coat\",         # index 4\n",
    "                        \"Sandal\",       # index 5\n",
    "                        \"Shirt\",        # index 6\n",
    "                        \"Sneaker\",      # index 7\n",
    "                        \"Bag\",          # index 8\n",
    "                        \"Ankle boot\"]   # index 9\n",
    " \n",
    "img_index = 5\n",
    " \n",
    "label_index = Y_train[img_index]\n",
    " \n",
    "print (\"Y = \" + str(label_index) + \" \" +(fashion_mnist_labels[label_index]))\n",
    " \n",
    "plt.imshow(X_train[img_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_mlp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   1,   0,   0,  13,  73,   0,   0,   1,\n",
       "         4,   0,   0,   0,   0,   1,   1,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   3,   0,  36, 136, 127,  62,\n",
       "        54,   0,   0,   0,   1,   3,   4,   0,   0,   3,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0, 102, 204,\n",
       "       176, 134, 144, 123,  23,   0,   0,   0,   0,  12,  10,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,  72,\n",
       "        15,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "         0,  69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,\n",
       "        88, 172,  66,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "         1,   1,   0, 200, 232, 232, 233, 229, 223, 223, 215, 213, 164,\n",
       "       127, 123, 196, 229,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 183, 225, 216, 223, 228, 235, 227, 224,\n",
       "       222, 224, 221, 223, 245, 173,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 193, 228, 218, 213, 198, 180,\n",
       "       212, 210, 211, 213, 223, 220, 243, 202,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   1,   3,   0,  12, 219, 220, 212, 218,\n",
       "       192, 169, 227, 208, 218, 224, 212, 226, 197, 209,  52,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99, 244, 222,\n",
       "       220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119, 167,  56,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
       "       236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
       "        92,   0,   0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,\n",
       "         0, 237, 226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215,\n",
       "       218, 255,  77,   0,   0,   3,   0,   0,   0,   0,   0,   0,   0,\n",
       "        62, 145, 204, 228, 207, 213, 221, 218, 208, 211, 218, 224, 223,\n",
       "       219, 215, 224, 244, 159,   0,   0,   0,   0,   0,  18,  44,  82,\n",
       "       107, 189, 228, 220, 222, 217, 226, 200, 205, 211, 230, 224, 234,\n",
       "       176, 188, 250, 248, 233, 238, 215,   0,   0,  57, 187, 208, 224,\n",
       "       221, 224, 208, 204, 214, 208, 209, 200, 159, 245, 193, 206, 223,\n",
       "       255, 255, 221, 234, 221, 211, 220, 232, 246,   0,   3, 202, 228,\n",
       "       224, 221, 211, 211, 214, 205, 205, 205, 220, 240,  80, 150, 255,\n",
       "       229, 221, 188, 154, 191, 210, 204, 209, 222, 228, 225,   0,  98,\n",
       "       233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217, 241,\n",
       "        65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224, 229,\n",
       "        29,  75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206,\n",
       "       198, 213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220,\n",
       "       221, 230,  67,  48, 203, 183, 194, 213, 197, 185, 190, 194, 192,\n",
       "       202, 214, 219, 221, 220, 236, 225, 216, 199, 206, 186, 181, 177,\n",
       "       172, 181, 205, 206, 115,   0, 122, 219, 193, 179, 171, 183, 196,\n",
       "       204, 210, 213, 207, 211, 210, 200, 196, 194, 191, 195, 191, 198,\n",
       "       192, 176, 156, 167, 177, 210,  92,   0,   0,  74, 189, 212, 191,\n",
       "       175, 172, 175, 181, 185, 188, 189, 188, 193, 198, 204, 209, 210,\n",
       "       210, 211, 188, 188, 194, 192, 216, 170,   0,   2,   0,   0,   0,\n",
       "        66, 200, 222, 237, 239, 242, 246, 243, 244, 221, 220, 193, 191,\n",
       "       179, 182, 182, 181, 176, 166, 168,  99,  58,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mlp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mlp = X_train_mlp.astype('float32')\n",
    "X_test_mlp = X_test_mlp.astype('float32')\n",
    "X_train_mlp /= 255\n",
    "X_test_mlp /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00392157, 0.        , 0.        , 0.05098039,\n",
       "       0.28627452, 0.        , 0.        , 0.00392157, 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "       0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
       "       0.        , 0.14117648, 0.53333336, 0.49803922, 0.24313726,\n",
       "       0.21176471, 0.        , 0.        , 0.        , 0.00392157,\n",
       "       0.01176471, 0.01568628, 0.        , 0.        , 0.01176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.02352941, 0.        , 0.4       ,\n",
       "       0.8       , 0.6901961 , 0.5254902 , 0.5647059 , 0.48235294,\n",
       "       0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.04705882, 0.03921569, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.60784316, 0.9254902 , 0.8117647 ,\n",
       "       0.69803923, 0.41960785, 0.6117647 , 0.6313726 , 0.42745098,\n",
       "       0.2509804 , 0.09019608, 0.3019608 , 0.50980395, 0.28235295,\n",
       "       0.05882353, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.        , 0.27058825,\n",
       "       0.8117647 , 0.8745098 , 0.85490197, 0.84705883, 0.84705883,\n",
       "       0.6392157 , 0.49803922, 0.4745098 , 0.47843137, 0.57254905,\n",
       "       0.5529412 , 0.34509805, 0.6745098 , 0.25882354, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00392157, 0.00392157,\n",
       "       0.00392157, 0.        , 0.78431374, 0.9098039 , 0.9098039 ,\n",
       "       0.9137255 , 0.8980392 , 0.8745098 , 0.8745098 , 0.84313726,\n",
       "       0.8352941 , 0.6431373 , 0.49803922, 0.48235294, 0.76862746,\n",
       "       0.8980392 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.7176471 , 0.88235295, 0.84705883, 0.8745098 , 0.89411765,\n",
       "       0.92156863, 0.8901961 , 0.8784314 , 0.87058824, 0.8784314 ,\n",
       "       0.8666667 , 0.8745098 , 0.9607843 , 0.6784314 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.75686276, 0.89411765,\n",
       "       0.85490197, 0.8352941 , 0.7764706 , 0.7058824 , 0.83137256,\n",
       "       0.8235294 , 0.827451  , 0.8352941 , 0.8745098 , 0.8627451 ,\n",
       "       0.9529412 , 0.7921569 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.01176471, 0.        ,\n",
       "       0.04705882, 0.85882354, 0.8627451 , 0.83137256, 0.85490197,\n",
       "       0.7529412 , 0.6627451 , 0.8901961 , 0.8156863 , 0.85490197,\n",
       "       0.8784314 , 0.83137256, 0.8862745 , 0.77254903, 0.81960785,\n",
       "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.02352941, 0.        , 0.3882353 , 0.95686275,\n",
       "       0.87058824, 0.8627451 , 0.85490197, 0.79607844, 0.7764706 ,\n",
       "       0.8666667 , 0.84313726, 0.8352941 , 0.87058824, 0.8627451 ,\n",
       "       0.9607843 , 0.46666667, 0.654902  , 0.21960784, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01568628, 0.        ,\n",
       "       0.        , 0.21568628, 0.9254902 , 0.89411765, 0.9019608 ,\n",
       "       0.89411765, 0.9411765 , 0.9098039 , 0.8352941 , 0.85490197,\n",
       "       0.8745098 , 0.91764706, 0.8509804 , 0.8509804 , 0.81960785,\n",
       "       0.36078432, 0.        , 0.        , 0.        , 0.00392157,\n",
       "       0.01568628, 0.02352941, 0.02745098, 0.00784314, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.92941177,\n",
       "       0.8862745 , 0.8509804 , 0.8745098 , 0.87058824, 0.85882354,\n",
       "       0.87058824, 0.8666667 , 0.84705883, 0.8745098 , 0.8980392 ,\n",
       "       0.84313726, 0.85490197, 1.        , 0.3019608 , 0.        ,\n",
       "       0.        , 0.01176471, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.24313726,\n",
       "       0.5686275 , 0.8       , 0.89411765, 0.8117647 , 0.8352941 ,\n",
       "       0.8666667 , 0.85490197, 0.8156863 , 0.827451  , 0.85490197,\n",
       "       0.8784314 , 0.8745098 , 0.85882354, 0.84313726, 0.8784314 ,\n",
       "       0.95686275, 0.62352943, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.17254902, 0.32156864,\n",
       "       0.41960785, 0.7411765 , 0.89411765, 0.8627451 , 0.87058824,\n",
       "       0.8509804 , 0.8862745 , 0.78431374, 0.8039216 , 0.827451  ,\n",
       "       0.9019608 , 0.8784314 , 0.91764706, 0.6901961 , 0.7372549 ,\n",
       "       0.98039216, 0.972549  , 0.9137255 , 0.93333334, 0.84313726,\n",
       "       0.        , 0.        , 0.22352941, 0.73333335, 0.8156863 ,\n",
       "       0.8784314 , 0.8666667 , 0.8784314 , 0.8156863 , 0.8       ,\n",
       "       0.8392157 , 0.8156863 , 0.81960785, 0.78431374, 0.62352943,\n",
       "       0.9607843 , 0.75686276, 0.80784315, 0.8745098 , 1.        ,\n",
       "       1.        , 0.8666667 , 0.91764706, 0.8666667 , 0.827451  ,\n",
       "       0.8627451 , 0.9098039 , 0.9647059 , 0.        , 0.01176471,\n",
       "       0.7921569 , 0.89411765, 0.8784314 , 0.8666667 , 0.827451  ,\n",
       "       0.827451  , 0.8392157 , 0.8039216 , 0.8039216 , 0.8039216 ,\n",
       "       0.8627451 , 0.9411765 , 0.3137255 , 0.5882353 , 1.        ,\n",
       "       0.8980392 , 0.8666667 , 0.7372549 , 0.6039216 , 0.7490196 ,\n",
       "       0.8235294 , 0.8       , 0.81960785, 0.87058824, 0.89411765,\n",
       "       0.88235295, 0.        , 0.38431373, 0.9137255 , 0.7764706 ,\n",
       "       0.8235294 , 0.87058824, 0.8980392 , 0.8980392 , 0.91764706,\n",
       "       0.9764706 , 0.8627451 , 0.7607843 , 0.84313726, 0.8509804 ,\n",
       "       0.94509804, 0.25490198, 0.28627452, 0.41568628, 0.45882353,\n",
       "       0.65882355, 0.85882354, 0.8666667 , 0.84313726, 0.8509804 ,\n",
       "       0.8745098 , 0.8745098 , 0.8784314 , 0.8980392 , 0.11372549,\n",
       "       0.29411766, 0.8       , 0.83137256, 0.8       , 0.75686276,\n",
       "       0.8039216 , 0.827451  , 0.88235295, 0.84705883, 0.7254902 ,\n",
       "       0.77254903, 0.80784315, 0.7764706 , 0.8352941 , 0.9411765 ,\n",
       "       0.7647059 , 0.8901961 , 0.9607843 , 0.9372549 , 0.8745098 ,\n",
       "       0.85490197, 0.83137256, 0.81960785, 0.87058824, 0.8627451 ,\n",
       "       0.8666667 , 0.9019608 , 0.2627451 , 0.1882353 , 0.79607844,\n",
       "       0.7176471 , 0.7607843 , 0.8352941 , 0.77254903, 0.7254902 ,\n",
       "       0.74509805, 0.7607843 , 0.7529412 , 0.7921569 , 0.8392157 ,\n",
       "       0.85882354, 0.8666667 , 0.8627451 , 0.9254902 , 0.88235295,\n",
       "       0.84705883, 0.78039217, 0.80784315, 0.7294118 , 0.70980394,\n",
       "       0.69411767, 0.6745098 , 0.70980394, 0.8039216 , 0.80784315,\n",
       "       0.4509804 , 0.        , 0.47843137, 0.85882354, 0.75686276,\n",
       "       0.7019608 , 0.67058825, 0.7176471 , 0.76862746, 0.8       ,\n",
       "       0.8235294 , 0.8352941 , 0.8117647 , 0.827451  , 0.8235294 ,\n",
       "       0.78431374, 0.76862746, 0.7607843 , 0.7490196 , 0.7647059 ,\n",
       "       0.7490196 , 0.7764706 , 0.7529412 , 0.6901961 , 0.6117647 ,\n",
       "       0.654902  , 0.69411767, 0.8235294 , 0.36078432, 0.        ,\n",
       "       0.        , 0.2901961 , 0.7411765 , 0.83137256, 0.7490196 ,\n",
       "       0.6862745 , 0.6745098 , 0.6862745 , 0.70980394, 0.7254902 ,\n",
       "       0.7372549 , 0.7411765 , 0.7372549 , 0.75686276, 0.7764706 ,\n",
       "       0.8       , 0.81960785, 0.8235294 , 0.8235294 , 0.827451  ,\n",
       "       0.7372549 , 0.7372549 , 0.7607843 , 0.7529412 , 0.84705883,\n",
       "       0.6666667 , 0.        , 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.25882354, 0.78431374, 0.87058824, 0.92941177,\n",
       "       0.9372549 , 0.9490196 , 0.9647059 , 0.9529412 , 0.95686275,\n",
       "       0.8666667 , 0.8627451 , 0.75686276, 0.7490196 , 0.7019608 ,\n",
       "       0.7137255 , 0.7137255 , 0.70980394, 0.6901961 , 0.6509804 ,\n",
       "       0.65882355, 0.3882353 , 0.22745098, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,\n",
       "       0.28235295, 0.16078432, 0.13725491, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mlp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_mlp = keras.utils.to_categorical(Y_train_mlp, num_classes)\n",
    "Y_test_mlp = keras.utils.to_categorical(Y_test_mlp, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_mlp[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mlp, X_val_mlp, Y_train_mlp, Y_val_mlp = train_test_split(X_train_mlp, Y_train_mlp, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_mlp.shape)\n",
    "print(X_val_mlp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import fashion_mnist\n",
    "#from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 625)               490625    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                6260      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 496885 (1.90 MB)\n",
      "Trainable params: 496885 (1.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(input_dim = 784, activation='sigmoid', units=625, kernel_initializer='normal'))\n",
    "\n",
    "model.add(Dense(input_dim = 625, activation='softmax', units=10, kernel_initializer='normal'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.05), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\sneha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\sneha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 1.3797 - accuracy: 0.6360 - val_loss: 0.9868 - val_accuracy: 0.7012\n",
      "Epoch 2/50\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.8535 - accuracy: 0.7346 - val_loss: 0.7882 - val_accuracy: 0.7344\n",
      "Epoch 3/50\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.7277 - accuracy: 0.7566 - val_loss: 0.7208 - val_accuracy: 0.7487\n",
      "Epoch 4/50\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.6666 - accuracy: 0.7711 - val_loss: 0.6674 - val_accuracy: 0.7677\n",
      "Epoch 5/50\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.6284 - accuracy: 0.7826 - val_loss: 0.6291 - val_accuracy: 0.7781\n",
      "Epoch 6/50\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.5998 - accuracy: 0.7913 - val_loss: 0.6022 - val_accuracy: 0.7908\n",
      "Epoch 7/50\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.5782 - accuracy: 0.7988 - val_loss: 0.5832 - val_accuracy: 0.7986\n",
      "Epoch 8/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.5601 - accuracy: 0.8051 - val_loss: 0.5671 - val_accuracy: 0.8016\n",
      "Epoch 9/50\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.5459 - accuracy: 0.8098 - val_loss: 0.5556 - val_accuracy: 0.8064\n",
      "Epoch 10/50\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.5337 - accuracy: 0.8152 - val_loss: 0.5544 - val_accuracy: 0.8046\n",
      "Epoch 11/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.5231 - accuracy: 0.8176 - val_loss: 0.5349 - val_accuracy: 0.8128\n",
      "Epoch 12/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.5142 - accuracy: 0.8213 - val_loss: 0.5341 - val_accuracy: 0.8083\n",
      "Epoch 13/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.5069 - accuracy: 0.8234 - val_loss: 0.5171 - val_accuracy: 0.8161\n",
      "Epoch 14/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4991 - accuracy: 0.8255 - val_loss: 0.5231 - val_accuracy: 0.8135\n",
      "Epoch 15/50\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.4931 - accuracy: 0.8290 - val_loss: 0.5121 - val_accuracy: 0.8207\n",
      "Epoch 16/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.4887 - accuracy: 0.8296 - val_loss: 0.5017 - val_accuracy: 0.8222\n",
      "Epoch 17/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.4825 - accuracy: 0.8315 - val_loss: 0.4950 - val_accuracy: 0.8249\n",
      "Epoch 18/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.4782 - accuracy: 0.8331 - val_loss: 0.4911 - val_accuracy: 0.8259\n",
      "Epoch 19/50\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.4739 - accuracy: 0.8344 - val_loss: 0.4907 - val_accuracy: 0.8281\n",
      "Epoch 20/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.4697 - accuracy: 0.8359 - val_loss: 0.4926 - val_accuracy: 0.8270\n",
      "Epoch 21/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.4667 - accuracy: 0.8368 - val_loss: 0.4930 - val_accuracy: 0.8248\n",
      "Epoch 22/50\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.4627 - accuracy: 0.8392 - val_loss: 0.4781 - val_accuracy: 0.8314\n",
      "Epoch 23/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4598 - accuracy: 0.8400 - val_loss: 0.4830 - val_accuracy: 0.8300\n",
      "Epoch 24/50\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.4563 - accuracy: 0.8400 - val_loss: 0.4733 - val_accuracy: 0.8313\n",
      "Epoch 25/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4540 - accuracy: 0.8417 - val_loss: 0.4733 - val_accuracy: 0.8325\n",
      "Epoch 26/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4509 - accuracy: 0.8423 - val_loss: 0.4721 - val_accuracy: 0.8357\n",
      "Epoch 27/50\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 0.4487 - accuracy: 0.8432 - val_loss: 0.4719 - val_accuracy: 0.8324\n",
      "Epoch 28/50\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 0.4462 - accuracy: 0.8454 - val_loss: 0.4667 - val_accuracy: 0.8363\n",
      "Epoch 29/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.4438 - accuracy: 0.8462 - val_loss: 0.4739 - val_accuracy: 0.8301\n",
      "Epoch 30/50\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.4410 - accuracy: 0.8460 - val_loss: 0.4653 - val_accuracy: 0.8381\n",
      "Epoch 31/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4395 - accuracy: 0.8462 - val_loss: 0.4575 - val_accuracy: 0.8394\n",
      "Epoch 32/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.4377 - accuracy: 0.8478 - val_loss: 0.4560 - val_accuracy: 0.8396\n",
      "Epoch 33/50\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 0.4354 - accuracy: 0.8480 - val_loss: 0.4562 - val_accuracy: 0.8393\n",
      "Epoch 34/50\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.4334 - accuracy: 0.8486 - val_loss: 0.4524 - val_accuracy: 0.8418\n",
      "Epoch 35/50\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 0.4317 - accuracy: 0.8493 - val_loss: 0.4499 - val_accuracy: 0.8425\n",
      "Epoch 36/50\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 0.4301 - accuracy: 0.8487 - val_loss: 0.4506 - val_accuracy: 0.8412\n",
      "Epoch 37/50\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 0.4283 - accuracy: 0.8508 - val_loss: 0.4490 - val_accuracy: 0.8423\n",
      "Epoch 38/50\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 0.4265 - accuracy: 0.8509 - val_loss: 0.4532 - val_accuracy: 0.8408\n",
      "Epoch 39/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4257 - accuracy: 0.8511 - val_loss: 0.4470 - val_accuracy: 0.8434\n",
      "Epoch 40/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4238 - accuracy: 0.8517 - val_loss: 0.4423 - val_accuracy: 0.8468\n",
      "Epoch 41/50\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 0.4219 - accuracy: 0.8530 - val_loss: 0.4459 - val_accuracy: 0.8456\n",
      "Epoch 42/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4208 - accuracy: 0.8528 - val_loss: 0.4430 - val_accuracy: 0.8451\n",
      "Epoch 43/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4194 - accuracy: 0.8537 - val_loss: 0.4400 - val_accuracy: 0.8481\n",
      "Epoch 44/50\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.4179 - accuracy: 0.8540 - val_loss: 0.4424 - val_accuracy: 0.8478\n",
      "Epoch 45/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4170 - accuracy: 0.8535 - val_loss: 0.4517 - val_accuracy: 0.8425\n",
      "Epoch 46/50\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.4158 - accuracy: 0.8539 - val_loss: 0.4383 - val_accuracy: 0.8481\n",
      "Epoch 47/50\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.4137 - accuracy: 0.8553 - val_loss: 0.4355 - val_accuracy: 0.8494\n",
      "Epoch 48/50\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.4130 - accuracy: 0.8555 - val_loss: 0.4384 - val_accuracy: 0.8467\n",
      "Epoch 49/50\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.4122 - accuracy: 0.8553 - val_loss: 0.4343 - val_accuracy: 0.8480\n",
      "Epoch 50/50\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.4106 - accuracy: 0.8559 - val_loss: 0.4383 - val_accuracy: 0.8479\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_mlp, Y_train_mlp,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val_mlp, Y_val_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4548 - accuracy: 0.8384\n",
      "\n",
      "MLP Test Loss: 0.45481958985328674\n",
      "MLP Test Accuracy: 0.8384000062942505\n"
     ]
    }
   ],
   "source": [
    "score  = model.evaluate(X_test_mlp, Y_test_mlp, verbose = 1)\n",
    "print()\n",
    "print('MLP Test Loss:' , score[0])\n",
    "print('MLP Test Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Multi Layer Perceptron"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
